{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatData(Dataset):\n",
    "    def __init__(self, path:str, tokenizer):\n",
    "        self.data = json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for i in self.data:\n",
    "            for j in i['dialog']:\n",
    "                self.X.append(j['text'])\n",
    "\n",
    "        for idx, i in enumerate(self.X):\n",
    "            try:\n",
    "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        self.X = self.X[:5000]\n",
    "        \n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<startofstring>\",\n",
    "                                \"eos_token\": \"<endofstring>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> I love iphone! i just bought new iphone! <bot>: Thats good for you, i'm not very into new tech <endofstring>\n"
     ]
    }
   ],
   "source": [
    "chatData = ChatData(\"./chat_data.json\", tokenizer)\n",
    "chatData =  DataLoader(chatData, batch_size=64)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "    inp = \"<startofstring> \"+inp+\" <bot>: \"\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(X, attention_mask=a )\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(chatData, model, optim):\n",
    "\n",
    "    epochs = 12\n",
    "\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        c = 1\n",
    "        for X, a in chatData:\n",
    "            X = X.to(device)\n",
    "            a = a.to(device)\n",
    "            optim.zero_grad()\n",
    "            loss = model(X, attention_mask=a, labels=X).loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            print(f\"epoch {i+1} batch {c} loss : {loss.item()}\")\n",
    "            c+=1\n",
    "        torch.save(model.state_dict(), \"model_state.pt\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(infer(\"hey\"))\n",
    "        print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 1 loss : 93.96897888183594\n",
      "epoch 1 batch 2 loss : 97.72197723388672\n",
      "epoch 1 batch 3 loss : 7.878519535064697\n",
      "epoch 1 batch 4 loss : 6.835137844085693\n",
      "epoch 1 batch 5 loss : 6.4496331214904785\n",
      "epoch 1 batch 6 loss : 6.296763896942139\n",
      "epoch 1 batch 7 loss : 5.942861080169678\n",
      "epoch 1 batch 8 loss : 5.4540324211120605\n",
      "epoch 1 batch 9 loss : 5.476049423217773\n",
      "epoch 1 batch 10 loss : 5.293665409088135\n",
      "epoch 1 batch 11 loss : 4.425160884857178\n",
      "epoch 1 batch 12 loss : 5.463339805603027\n",
      "epoch 1 batch 13 loss : 4.793827056884766\n",
      "epoch 1 batch 14 loss : 4.37387752532959\n",
      "epoch 1 batch 15 loss : 3.198378562927246\n",
      "epoch 1 batch 16 loss : 3.140977144241333\n",
      "epoch 1 batch 17 loss : 4.76219367980957\n",
      "epoch 1 batch 18 loss : 4.2240891456604\n",
      "epoch 1 batch 19 loss : 4.558444499969482\n",
      "epoch 1 batch 20 loss : 4.790134906768799\n",
      "epoch 1 batch 21 loss : 4.475421905517578\n",
      "epoch 1 batch 22 loss : 4.625162601470947\n",
      "epoch 1 batch 23 loss : 3.4931232929229736\n",
      "epoch 1 batch 24 loss : 3.7245404720306396\n",
      "epoch 1 batch 25 loss : 4.79451847076416\n",
      "epoch 1 batch 26 loss : 3.485197067260742\n",
      "epoch 1 batch 27 loss : 3.490304708480835\n",
      "epoch 1 batch 28 loss : 4.056334972381592\n",
      "epoch 1 batch 29 loss : 3.670997142791748\n",
      "epoch 1 batch 30 loss : 3.417341947555542\n",
      "epoch 1 batch 31 loss : 3.2150332927703857\n",
      "epoch 1 batch 32 loss : 2.987262010574341\n",
      "epoch 1 batch 33 loss : 3.3415918350219727\n",
      "epoch 1 batch 34 loss : 2.913034439086914\n",
      "epoch 1 batch 35 loss : 2.9244461059570312\n",
      "epoch 1 batch 36 loss : 4.075028419494629\n",
      "epoch 1 batch 37 loss : 3.795271158218384\n",
      "epoch 1 batch 38 loss : 3.1407060623168945\n",
      "epoch 1 batch 39 loss : 2.5598275661468506\n",
      "epoch 1 batch 40 loss : 2.8671793937683105\n",
      "epoch 1 batch 41 loss : 2.7979958057403564\n",
      "epoch 1 batch 42 loss : 2.4124958515167236\n",
      "epoch 1 batch 43 loss : 2.6944401264190674\n",
      "epoch 1 batch 44 loss : 2.815135955810547\n",
      "epoch 1 batch 45 loss : 2.241429567337036\n",
      "epoch 1 batch 46 loss : 2.755812168121338\n",
      "epoch 1 batch 47 loss : 3.731781005859375\n",
      "epoch 1 batch 48 loss : 2.3819456100463867\n",
      "epoch 1 batch 49 loss : 3.718816041946411\n",
      "epoch 1 batch 50 loss : 3.574190855026245\n",
      "epoch 1 batch 51 loss : 3.19396710395813\n",
      "epoch 1 batch 52 loss : 2.5546836853027344\n",
      "epoch 1 batch 53 loss : 1.2016685009002686\n",
      "epoch 1 batch 54 loss : 2.1972334384918213\n",
      "epoch 1 batch 55 loss : 2.1641170978546143\n",
      "epoch 1 batch 56 loss : 2.1889748573303223\n",
      "epoch 1 batch 57 loss : 1.8272734880447388\n",
      "epoch 1 batch 58 loss : 2.0525476932525635\n",
      "epoch 1 batch 59 loss : 2.366349697113037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 1/12 [14:43<2:41:58, 883.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "<startofstring> hey  <bot>:  <endofstring><|endoftext|>\n",
      "--------------------------------------------------\n",
      "epoch 2 batch 1 loss : 2.5716030597686768\n",
      "epoch 2 batch 2 loss : 2.831345558166504\n",
      "epoch 2 batch 3 loss : 2.5584511756896973\n",
      "epoch 2 batch 4 loss : 2.516566514968872\n",
      "epoch 2 batch 5 loss : 2.5286357402801514\n",
      "epoch 2 batch 6 loss : 2.772916555404663\n",
      "epoch 2 batch 7 loss : 2.4544241428375244\n",
      "epoch 2 batch 8 loss : 2.253345012664795\n",
      "epoch 2 batch 9 loss : 2.530212879180908\n",
      "epoch 2 batch 10 loss : 2.76501727104187\n",
      "epoch 2 batch 11 loss : 2.1326985359191895\n",
      "epoch 2 batch 12 loss : 3.179919481277466\n",
      "epoch 2 batch 13 loss : 2.4909563064575195\n",
      "epoch 2 batch 14 loss : 2.133256673812866\n",
      "epoch 2 batch 15 loss : 1.5712839365005493\n",
      "epoch 2 batch 16 loss : 1.5368993282318115\n",
      "epoch 2 batch 17 loss : 3.087360143661499\n",
      "epoch 2 batch 18 loss : 2.49908447265625\n",
      "epoch 2 batch 19 loss : 2.976935863494873\n",
      "epoch 2 batch 20 loss : 3.557049036026001\n",
      "epoch 2 batch 21 loss : 3.0457205772399902\n",
      "epoch 2 batch 22 loss : 3.3662567138671875\n",
      "epoch 2 batch 23 loss : 2.3410868644714355\n",
      "epoch 2 batch 24 loss : 2.6612932682037354\n",
      "epoch 2 batch 25 loss : 3.8973686695098877\n",
      "epoch 2 batch 26 loss : 2.587188243865967\n",
      "epoch 2 batch 27 loss : 2.454392910003662\n",
      "epoch 2 batch 28 loss : 3.1591601371765137\n",
      "epoch 2 batch 29 loss : 2.834345579147339\n",
      "epoch 2 batch 30 loss : 2.609811782836914\n",
      "epoch 2 batch 31 loss : 2.413144111633301\n",
      "epoch 2 batch 32 loss : 2.2369322776794434\n",
      "epoch 2 batch 33 loss : 2.494234800338745\n",
      "epoch 2 batch 34 loss : 2.1828696727752686\n",
      "epoch 2 batch 35 loss : 2.2051284313201904\n",
      "epoch 2 batch 36 loss : 3.2378454208374023\n",
      "epoch 2 batch 37 loss : 3.0577523708343506\n",
      "epoch 2 batch 38 loss : 2.558865785598755\n",
      "epoch 2 batch 39 loss : 2.0217387676239014\n",
      "epoch 2 batch 40 loss : 2.2576065063476562\n",
      "epoch 2 batch 41 loss : 2.249450922012329\n",
      "epoch 2 batch 42 loss : 1.9112842082977295\n",
      "epoch 2 batch 43 loss : 2.1811583042144775\n",
      "epoch 2 batch 44 loss : 2.305266857147217\n",
      "epoch 2 batch 45 loss : 1.8131215572357178\n",
      "epoch 2 batch 46 loss : 2.2184596061706543\n",
      "epoch 2 batch 47 loss : 3.127393960952759\n",
      "epoch 2 batch 48 loss : 1.9144219160079956\n",
      "epoch 2 batch 49 loss : 3.1095046997070312\n",
      "epoch 2 batch 50 loss : 2.9969635009765625\n",
      "epoch 2 batch 51 loss : 2.6283888816833496\n",
      "epoch 2 batch 52 loss : 2.0527572631835938\n",
      "epoch 2 batch 53 loss : 0.8660681247711182\n",
      "epoch 2 batch 54 loss : 1.7792071104049683\n",
      "epoch 2 batch 55 loss : 1.7474606037139893\n",
      "epoch 2 batch 56 loss : 1.7622641324996948\n",
      "epoch 2 batch 57 loss : 1.417109489440918\n",
      "epoch 2 batch 58 loss : 1.5860483646392822\n",
      "epoch 2 batch 59 loss : 1.7951579093933105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 2/12 [28:17<2:20:24, 842.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "<startofstring> hey  <bot>:  <endofstring><|endoftext|>\n",
      "--------------------------------------------------\n",
      "epoch 3 batch 1 loss : 2.167609691619873\n",
      "epoch 3 batch 2 loss : 2.477191209793091\n",
      "epoch 3 batch 3 loss : 2.260333299636841\n",
      "epoch 3 batch 4 loss : 2.1466963291168213\n",
      "epoch 3 batch 5 loss : 2.1125950813293457\n",
      "epoch 3 batch 6 loss : 2.1777217388153076\n",
      "epoch 3 batch 7 loss : 2.039309024810791\n",
      "epoch 3 batch 8 loss : 1.769187331199646\n",
      "epoch 3 batch 9 loss : 2.0679585933685303\n",
      "epoch 3 batch 10 loss : 2.3294169902801514\n",
      "epoch 3 batch 11 loss : 1.768523931503296\n",
      "epoch 3 batch 12 loss : 2.6785929203033447\n",
      "epoch 3 batch 13 loss : 2.069905996322632\n",
      "epoch 3 batch 14 loss : 1.6633081436157227\n",
      "epoch 3 batch 15 loss : 1.351298451423645\n",
      "epoch 3 batch 16 loss : 1.1858493089675903\n",
      "epoch 3 batch 17 loss : 2.536935567855835\n",
      "epoch 3 batch 18 loss : 1.9535653591156006\n",
      "epoch 3 batch 19 loss : 2.466430902481079\n",
      "epoch 3 batch 20 loss : 3.0759754180908203\n",
      "epoch 3 batch 21 loss : 2.5440099239349365\n",
      "epoch 3 batch 22 loss : 2.852764368057251\n",
      "epoch 3 batch 23 loss : 1.8286768198013306\n",
      "epoch 3 batch 24 loss : 2.0938544273376465\n",
      "epoch 3 batch 25 loss : 3.1926112174987793\n",
      "epoch 3 batch 26 loss : 2.1566853523254395\n",
      "epoch 3 batch 27 loss : 1.9801846742630005\n",
      "epoch 3 batch 28 loss : 2.653393030166626\n",
      "epoch 3 batch 29 loss : 2.2729411125183105\n",
      "epoch 3 batch 30 loss : 2.12150239944458\n",
      "epoch 3 batch 31 loss : 1.914936900138855\n",
      "epoch 3 batch 32 loss : 1.7231345176696777\n",
      "epoch 3 batch 33 loss : 2.0057661533355713\n",
      "epoch 3 batch 34 loss : 1.7478525638580322\n",
      "epoch 3 batch 35 loss : 1.712837815284729\n",
      "epoch 3 batch 36 loss : 2.601736307144165\n",
      "epoch 3 batch 37 loss : 2.536993980407715\n",
      "epoch 3 batch 38 loss : 2.074971914291382\n",
      "epoch 3 batch 39 loss : 1.6107029914855957\n",
      "epoch 3 batch 40 loss : 1.8327966928482056\n",
      "epoch 3 batch 41 loss : 1.859410047531128\n",
      "epoch 3 batch 42 loss : 1.610162615776062\n",
      "epoch 3 batch 43 loss : 1.7417291402816772\n",
      "epoch 3 batch 44 loss : 1.9027036428451538\n",
      "epoch 3 batch 45 loss : 1.520559310913086\n",
      "epoch 3 batch 46 loss : 1.7505124807357788\n",
      "epoch 3 batch 47 loss : 2.6494784355163574\n",
      "epoch 3 batch 48 loss : 1.4773283004760742\n",
      "epoch 3 batch 49 loss : 2.569136381149292\n",
      "epoch 3 batch 50 loss : 2.5728631019592285\n",
      "epoch 3 batch 51 loss : 2.2007687091827393\n",
      "epoch 3 batch 52 loss : 1.6969918012619019\n",
      "epoch 3 batch 53 loss : 0.6451503038406372\n",
      "epoch 3 batch 54 loss : 1.4544153213500977\n",
      "epoch 3 batch 55 loss : 1.4151822328567505\n",
      "epoch 3 batch 56 loss : 1.4104366302490234\n",
      "epoch 3 batch 57 loss : 1.1930397748947144\n",
      "epoch 3 batch 58 loss : 1.264482021331787\n",
      "epoch 3 batch 59 loss : 1.3917343616485596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [41:44<2:03:57, 826.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hey  <bot>:   <bot>:  Hi, i am a huge gamer <endofstring><|endoftext|>\n",
      "--------------------------------------------------\n",
      "epoch 4 batch 1 loss : 1.781527042388916\n",
      "epoch 4 batch 2 loss : 2.164747953414917\n",
      "epoch 4 batch 3 loss : 1.9329111576080322\n",
      "epoch 4 batch 4 loss : 1.81088125705719\n",
      "epoch 4 batch 5 loss : 1.7368439435958862\n",
      "epoch 4 batch 6 loss : 1.6519947052001953\n",
      "epoch 4 batch 7 loss : 1.706878662109375\n",
      "epoch 4 batch 8 loss : 1.495866298675537\n",
      "epoch 4 batch 9 loss : 1.7077736854553223\n",
      "epoch 4 batch 10 loss : 1.9112392663955688\n",
      "epoch 4 batch 11 loss : 1.495490550994873\n",
      "epoch 4 batch 12 loss : 2.2232449054718018\n",
      "epoch 4 batch 13 loss : 1.7365000247955322\n",
      "epoch 4 batch 14 loss : 1.3113024234771729\n",
      "epoch 4 batch 15 loss : 1.1036690473556519\n",
      "epoch 4 batch 16 loss : 0.9549606442451477\n",
      "epoch 4 batch 17 loss : 2.069035768508911\n",
      "epoch 4 batch 18 loss : 1.5575538873672485\n",
      "epoch 4 batch 19 loss : 2.0563242435455322\n",
      "epoch 4 batch 20 loss : 2.5653860569000244\n",
      "epoch 4 batch 21 loss : 2.108509063720703\n",
      "epoch 4 batch 22 loss : 2.4123332500457764\n",
      "epoch 4 batch 23 loss : 1.42737877368927\n",
      "epoch 4 batch 24 loss : 1.6690908670425415\n",
      "epoch 4 batch 25 loss : 2.641996145248413\n",
      "epoch 4 batch 26 loss : 1.7888742685317993\n",
      "epoch 4 batch 27 loss : 1.5830967426300049\n",
      "epoch 4 batch 28 loss : 2.2272751331329346\n",
      "epoch 4 batch 29 loss : 1.8707754611968994\n",
      "epoch 4 batch 30 loss : 1.787841796875\n",
      "epoch 4 batch 31 loss : 1.5521228313446045\n",
      "epoch 4 batch 32 loss : 1.4401895999908447\n",
      "epoch 4 batch 33 loss : 1.5876883268356323\n",
      "epoch 4 batch 34 loss : 1.414828896522522\n",
      "epoch 4 batch 35 loss : 1.4569529294967651\n",
      "epoch 4 batch 36 loss : 2.141876697540283\n",
      "epoch 4 batch 37 loss : 2.1531872749328613\n",
      "epoch 4 batch 38 loss : 1.7193595170974731\n",
      "epoch 4 batch 39 loss : 1.302554965019226\n",
      "epoch 4 batch 40 loss : 1.539326786994934\n",
      "epoch 4 batch 41 loss : 1.6065599918365479\n",
      "epoch 4 batch 42 loss : 1.3698679208755493\n",
      "epoch 4 batch 43 loss : 1.4624134302139282\n",
      "epoch 4 batch 44 loss : 1.6286349296569824\n",
      "epoch 4 batch 45 loss : 1.2829763889312744\n",
      "epoch 4 batch 46 loss : 1.4467347860336304\n",
      "epoch 4 batch 47 loss : 2.2944886684417725\n",
      "epoch 4 batch 48 loss : 1.1994664669036865\n",
      "epoch 4 batch 49 loss : 2.2285408973693848\n",
      "epoch 4 batch 50 loss : 2.224278211593628\n",
      "epoch 4 batch 51 loss : 1.8311022520065308\n",
      "epoch 4 batch 52 loss : 1.4307688474655151\n",
      "epoch 4 batch 53 loss : 0.5138925313949585\n",
      "epoch 4 batch 54 loss : 1.2054392099380493\n",
      "epoch 4 batch 55 loss : 1.124854326248169\n",
      "epoch 4 batch 56 loss : 1.0982123613357544\n",
      "epoch 4 batch 57 loss : 1.021971344947815\n",
      "epoch 4 batch 58 loss : 1.0214790105819702\n",
      "epoch 4 batch 59 loss : 1.108372688293457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [55:27<1:49:59, 824.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hey  <bot>:   <bot>: <bot>:  Hi, how are doing? <endofstring><|endoftext|>\n",
      "--------------------------------------------------\n",
      "epoch 5 batch 1 loss : 1.5162222385406494\n",
      "epoch 5 batch 2 loss : 1.8197280168533325\n",
      "epoch 5 batch 3 loss : 1.678934097290039\n",
      "epoch 5 batch 4 loss : 1.560239315032959\n",
      "epoch 5 batch 5 loss : 1.4872900247573853\n",
      "epoch 5 batch 6 loss : 1.366298794746399\n",
      "epoch 5 batch 7 loss : 1.4529272317886353\n",
      "epoch 5 batch 8 loss : 1.2559969425201416\n",
      "epoch 5 batch 9 loss : 1.4105192422866821\n",
      "epoch 5 batch 10 loss : 1.6551755666732788\n",
      "epoch 5 batch 11 loss : 1.2214432954788208\n",
      "epoch 5 batch 12 loss : 1.9017025232315063\n",
      "epoch 5 batch 13 loss : 1.4584952592849731\n",
      "epoch 5 batch 14 loss : 1.0310217142105103\n",
      "epoch 5 batch 15 loss : 0.9085278511047363\n",
      "epoch 5 batch 16 loss : 0.7953083515167236\n",
      "epoch 5 batch 17 loss : 1.7287845611572266\n",
      "epoch 5 batch 18 loss : 1.2755613327026367\n",
      "epoch 5 batch 19 loss : 1.6845321655273438\n",
      "epoch 5 batch 20 loss : 2.0785109996795654\n",
      "epoch 5 batch 21 loss : 1.7826303243637085\n",
      "epoch 5 batch 22 loss : 2.0216174125671387\n",
      "epoch 5 batch 23 loss : 1.186143398284912\n",
      "epoch 5 batch 24 loss : 1.3981040716171265\n",
      "epoch 5 batch 25 loss : 2.2627296447753906\n",
      "epoch 5 batch 26 loss : 1.515759825706482\n",
      "epoch 5 batch 27 loss : 1.3296353816986084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [1:01:43<2:03:26, 925.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\kuliah\\sem6\\ai\\pertemuan-sekian\\tee-am-ai\\model\\trainer-fatwa\\model.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining .... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(chatData, model, optim)\n",
      "\u001b[1;32md:\\kuliah\\sem6\\ai\\pertemuan-sekian\\tee-am-ai\\model\\trainer-fatwa\\model.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m model(X, attention_mask\u001b[39m=\u001b[39;49ma, labels\u001b[39m=\u001b[39;49mX)\u001b[39m.\u001b[39mloss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/kuliah/sem6/ai/pertemuan-sekian/tee-am-ai/model/trainer-fatwa/model.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1302\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1302\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1303\u001b[0m     input_ids,\n\u001b[0;32m   1304\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1305\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1306\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1307\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1308\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1309\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1310\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1311\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1312\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1313\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1314\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1315\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1317\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1319\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1116\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1105\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m   1106\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         output_attentions,\n\u001b[0;32m   1114\u001b[0m     )\n\u001b[0;32m   1115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m   1117\u001b[0m         hidden_states,\n\u001b[0;32m   1118\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m   1119\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1120\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m   1121\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1122\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1123\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1124\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1125\u001b[0m     )\n\u001b[0;32m   1127\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    612\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 614\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    615\u001b[0m     hidden_states,\n\u001b[0;32m    616\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    617\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    618\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    619\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    620\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    621\u001b[0m )\n\u001b[0;32m    622\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    623\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:325\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[0;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    327\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    328\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\pytorch_utils.py:104\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    103\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[1;32m--> 104\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    105\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[0;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"training .... \")\n",
    "train(chatData, model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer from model : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hey  <bot>:  <startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring>\n",
      "Terminating the program...\n"
     ]
    }
   ],
   "source": [
    "print(\"infer from model : \")\n",
    "while True:\n",
    "  inp = input()\n",
    "  if inp.lower() == 'exit':\n",
    "    print(\"Terminating the program...\")\n",
    "    break\n",
    "\n",
    "  print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
